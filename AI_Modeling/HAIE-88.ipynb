{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code for classifying apps based on category\n",
    "\n",
    "#Required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "cwd = os.getcwd() #Copy current working directory\n",
    "\n",
    "#df = pd.read_csv(cwd + '/All Categories.csv',index_col=0  ) # Read csv\n",
    "df = pd.read_csv('/kaggle/input/google-play-store-category-wise-top-500-apps/All Categories.csv',index_col=0  ) # Read csv\n",
    "\n",
    "df.head() # First 5 rows\n",
    "\n",
    "data = df.copy() # Copy to another DataFrame\n",
    "\n",
    "data.info() # DataFrame information\n",
    "\n",
    "data = data[data.Reviews.notna()] # Drop NaN value\n",
    "\n",
    "data.info()\n",
    "\n",
    "data.drop('Size',axis=1, inplace=True) # Drop Size Column\n",
    "\n",
    "data.Reviews.unique() # Unique values of the column Reviews\n",
    "\n",
    "data.Downloads.unique() # Unique values of the column Downloads\n",
    "\n",
    "# Abbreviations\n",
    "\n",
    "# T = 1000            (K)\n",
    "# L = 100000 \n",
    "# Cr = 10000000       (10M)\n",
    "# TCr = 10000000000   (10B)\n",
    "\n",
    "data[data.Downloads.str.contains('TCr', regex=False)]   # YouTube only app with more than 10 Billion Downloads == 1TCr+\n",
    "\n",
    "### Changing abbreviations of Reviews\n",
    "\n",
    "##### Change current abbreviations to (K(Thousand), M(Million)) and (K(Thousand), M(Million), B(Billion)) in the case of Downloads\n",
    "#### Creating two columns, one with the new abbreviations and one in units.\n",
    "\n",
    "Lista_num = [] # Numbers saved as str\n",
    "Lista_str = [] # Abbreviations\n",
    "\n",
    "for value in data.Reviews:\n",
    "    if value.endswith('T') == True: \n",
    "        Lista_num.append(value[0:-1])\n",
    "        Lista_str.append(value[-1])\n",
    "    elif value.endswith('L') == True:\n",
    "        Lista_num.append(value[0:-1])\n",
    "        Lista_str.append(value[-1])\n",
    "    elif value.endswith('Cr') == True:\n",
    "        Lista_num.append(value[0:-2])\n",
    "        Lista_str.append(value[-2:])\n",
    "    else:\n",
    "        Lista_num.append(value)\n",
    "        Lista_str.append('')\n",
    "\n",
    "Lista_str # List of Abbreviations\n",
    "\n",
    "\n",
    "np.unique(np.array(Lista_str)) # Unique values of the list\n",
    "\n",
    "# Converting abbreviations to units\n",
    "\n",
    "# T = 1000     (K)\n",
    "# L = 100000 \n",
    "# Cr = 10000000       (10M)\n",
    "\n",
    "Lista_str_to_num = []\n",
    "\n",
    "for value in Lista_str:\n",
    "    if value == 'T': \n",
    "        Lista_str_to_num.append(1000)\n",
    "    elif value == 'L':\n",
    "        Lista_str_to_num.append(100000)\n",
    "    elif value == 'Cr':\n",
    "        Lista_str_to_num.append(10000000)\n",
    "    else:\n",
    "        Lista_str_to_num.append(1)\n",
    "\n",
    "\n",
    "Lista_str_to_num # List of numbers converted from the abb\n",
    "\n",
    "# Converting numbers saved as str to integers\n",
    "int_output = map(int, Lista_num) #Maps each string to an int.\n",
    "integer_list = list(int_output) #Converts mapped output to a list of ints.\n",
    "print(integer_list[0:20])\n",
    "\n",
    "Reviews_unit = np.array(Lista_str_to_num) * np.array(integer_list)\n",
    "Reviews_unit      # Reviews in units\n",
    "\n",
    "# Creating a List of Reviews with abbreviations (K(Thousand), M(Million))\n",
    "Reviews = []\n",
    "\n",
    "for v in Reviews_unit:\n",
    "    if v/1000000 >= 1:\n",
    "        Reviews.append(f'{round(v/1000000,1)}M')\n",
    "    elif v/1000 >= 1: \n",
    "        Reviews.append(f'{int(v/1000)}K')\n",
    "    else:\n",
    "        Reviews.append(f'{v}') \n",
    "\n",
    "Reviews        # Reviews with different abbreviattions \n",
    "\n",
    "np.unique(np.array(Reviews))\n",
    "\n",
    "Reviews_Abb = np.array(Reviews)  # Reviews with abbreviations saved as a numpy array \n",
    "\n",
    "### Changing abbreviations of Downloads\n",
    "\n",
    "# Same process of changing the abbreviations with the column of Downloads\n",
    "data.Downloads.unique()\n",
    "\n",
    "\n",
    "Lista_num_downloads = []\n",
    "Lista_str_downloads = []\n",
    "\n",
    "for value in data.Downloads:\n",
    "    if value.endswith('T+') == True: \n",
    "        Lista_num_downloads.append(value[0:-2])\n",
    "        Lista_str_downloads.append(value[-2:])\n",
    "    elif value.endswith('L+') == True:\n",
    "        Lista_num_downloads.append(value[0:-2])\n",
    "        Lista_str_downloads.append(value[-2:])\n",
    "    elif value.endswith('TCr+') == True:\n",
    "        Lista_num_downloads.append(value[0:-4])\n",
    "        Lista_str_downloads.append(value[-4:])\n",
    "    else:  #Cr+\n",
    "        Lista_num_downloads.append(value[0:-3])\n",
    "        Lista_str_downloads.append(value[-3:])\n",
    "  \n",
    "\n",
    "np.unique(np.array(Lista_str_downloads))\n",
    "\n",
    "# T = 1000            (K)\n",
    "# L = 100000 \n",
    "# Cr = 10000000       (10M)\n",
    "# TCr = 10000000000   10.000.000.000  (10B)\n",
    "\n",
    "Lista_str_to_num_downloads = []\n",
    "\n",
    "for value in Lista_str_downloads:\n",
    "    if value == 'T+': \n",
    "        Lista_str_to_num_downloads.append(1000)\n",
    "    elif value == 'L+':\n",
    "        Lista_str_to_num_downloads.append(100000)\n",
    "    elif value == 'TCr+':\n",
    "        Lista_str_to_num_downloads.append(10000000000)\n",
    "    else:   #Cr+\n",
    "        Lista_str_to_num_downloads.append(10000000)\n",
    "   \n",
    "\n",
    "np.unique(np.array(Lista_str_to_num_downloads))\n",
    "\n",
    "\n",
    "int_output = map(int, Lista_num_downloads) #Maps each string to an int.\n",
    "int_list = list(int_output) #Converts mapped output to a list of ints.\n",
    "print(int_list[0:20])\n",
    "\n",
    "Downloads_unit = np.array(Lista_str_to_num_downloads) * np.array(int_list)\n",
    "Downloads_unit     # array of Downloads in units\n",
    "\n",
    "Downloads = []\n",
    " \n",
    "for v in Downloads_unit:\n",
    "    if v/1000000000 >= 1:\n",
    "        Downloads.append(f'{int(v/1000000000)}B+')\n",
    "    elif v/1000000 >= 1:\n",
    "        Downloads.append(f'{int(v/1000000)}M+')\n",
    "    else: \n",
    "        Downloads.append(f'{int(v/1000)}K+')  \n",
    "\n",
    "Downloads        \n",
    "\n",
    "np.unique(np.array(Downloads)) # Unique Download values\n",
    "\n",
    "\n",
    "Downloads_Abb = np.array(Downloads)  # array of Downloads with new abbreviattions \n",
    "\n",
    "### Adding the new columns\n",
    "\n",
    "data.head()\n",
    "\n",
    "# Adding Review Columns\n",
    "data['Reviews_Abb'] = Reviews_Abb  \n",
    "data['Reviews_unit'] = Reviews_unit\n",
    "\n",
    "# Adding Download Columns\n",
    "data['Downloads_Abb'] = Downloads_Abb  \n",
    "data['Downloads_unit'] = Downloads_unit\n",
    "\n",
    "data.head()\n",
    "\n",
    "### Creating new Dataframe with complete data\n",
    "\n",
    "data.columns\n",
    "\n",
    "# New order of columns\n",
    "columns = ['Name', 'Developer', 'Category', 'Star Rating', 'Reviews_Abb', 'Reviews_unit', 'Downloads_Abb',\n",
    "       'Downloads_unit','Rated for']\n",
    "\n",
    "all_data = data[columns] # Creating new df called all_data\n",
    "\n",
    "all_data.rename(columns={'Star Rating':'Star_Rating','Rated for':'Rated_for'}, inplace=True) #rename columns with blank spaces\n",
    "\n",
    "all_data.head()\n",
    "\n",
    "## Visualizations\n",
    "\n",
    "### Rating Distribution\n",
    "\n",
    "plt.style.use('bmh')\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "media_rating =all_data.Star_Rating.mean() # \n",
    "\n",
    "plt.axvline(x = media_rating , color=\"b\", ls = '-',label=\"Average Rating\")\n",
    "\n",
    "plt.hist(all_data.Star_Rating, color='g')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.title(\"Star Rating Distribution\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Absolute frequency\")\n",
    "\n",
    "plt.legend(loc = \"upper left\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### Top 10 Categories by number of apps in the ranking\n",
    "\n",
    "category_count = all_data.Category.value_counts()[:10]\n",
    "category_count\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.barh(category_count.index,category_count , color='g')\n",
    "plt.title(\"Top 10 Categories by number of apps in the ranking\",{'fontsize':20},pad=20)\n",
    "plt.xlabel(\"Number of Apps\")\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### Correlation between Star Rating and Number of Downloads?\n",
    "\n",
    "fig = px.scatter(all_data, x=\"Star_Rating\", y=\"Downloads_unit\")\n",
    "fig.show()\n",
    "\n",
    "df_no_outliers = all_data[all_data.Downloads_unit <5000000000]   # Drop Outliers\n",
    "fig = px.scatter(df_no_outliers, x=\"Star_Rating\", y=\"Downloads_unit\")  \n",
    "fig.show()\n",
    "\n",
    "#    There is no correlation     Correlation ~ 0  \n",
    "# Numpy\n",
    "print(np.corrcoef(all_data.Star_Rating ,all_data.Downloads_unit)[0,1])\n",
    "# Pandas\n",
    "print(all_data.Star_Rating.corr(all_data.Downloads_unit)  )\n",
    "\n",
    "### Apps with most downloads \n",
    "\n",
    "# TOP 20 Apps with most downloads\n",
    "all_data.sort_values('Downloads_unit',ascending=False).iloc[0:20,np.r_[0:2,3,6]]\n",
    "\n",
    "### Top 10 Developers by number of Apps in Ranking\n",
    "\n",
    "apps_by_devs = all_data.Developer.value_counts()[:10]  # Number of apps in the ranking by Developers\n",
    "apps_by_devs\n",
    "\n",
    "apps_by_devs.index \n",
    "\n",
    "fig = px.bar(apps_by_devs, y=apps_by_devs.index , x=apps_by_devs, text=apps_by_devs, orientation='h')\n",
    "\n",
    "fig.update_traces(textfont_size=12, marker_color='green',textangle=0, textposition=\"inside\", cliponaxis=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text':\"Top 10 Developers by number of Apps in Ranking\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'} , \n",
    "    yaxis_title = None,\n",
    "    xaxis_title=\"Number of Apps\",\n",
    "    yaxis={'categoryorder':'total ascending'},\n",
    "    font=dict(\n",
    "        family=\"Arial, monospace\",\n",
    "        size=14,\n",
    "        color=\"Green\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "### Top 10 Developers by number of Downloads\n",
    "\n",
    "# Developers with the most Downloads\n",
    "Downloads_devs_10 = all_data.groupby(['Developer']).sum()['Downloads_unit'].sort_values(ascending=False)[0:10]\n",
    "Downloads_devs_10\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "ax = sns.barplot(x=Downloads_devs_10, y=Downloads_devs_10.index, ax=ax, color='g', errwidth=0)\n",
    "\n",
    "ax.set_title('Top 10 Developers by number of Downloads', {'fontsize':20},pad=20)\n",
    "ax.set_xlabel(\"Downloads (Tens of Billions)\",{'fontsize':14})\n",
    "ax.set(ylabel=None)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca42571",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'drive/MyDrive/All Categories.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m categories_df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrive/MyDrive/All Categories.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m victor_df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrive/MyDrive/Sample - Activities - Hoja 1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# from google.colab import drive\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# drive.mount('/content/drive')\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/All Categories.csv'"
     ]
    }
   ],
   "source": [
    "## code for filtering classification cases without a match \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "categories_df=pd.read_csv(\"drive/MyDrive/All Categories.csv\")\n",
    "victor_df=pd.read_csv(\"drive/MyDrive/Sample - Activities - Hoja 1.csv\")\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "print (categories_df.columns)\n",
    "\n",
    "categories_df=categories_df[['Name', 'Category']]\n",
    "categories_df.head()\n",
    "\n",
    "print (victor_df.columns)\n",
    "\n",
    "activity_switching_df=victor_df[victor_df['EventName']== 'FocusIn']\n",
    "activity_switching_df=activity_switching_df[['EventName','Parameters']]\n",
    "print (activity_switching_df)\n",
    "\n",
    "for item in activity_switching_df['Parameters']:\n",
    "  json_params=json.loads(item.strip(\"[]\"))\n",
    "  print (json_params)\n",
    "  app_name=json_params[\"AppName\"]\n",
    "  cat_item=categories_df[categories_df['Name']==app_name]\n",
    "  print (cat_item)\n",
    "\n",
    "print (categories_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code for matching apps to pre-labeled categories \n",
    "\n",
    "# Text Classification\n",
    "\n",
    "## 0 - Setup\n",
    "\n",
    "### Install libraries\n",
    "\n",
    "!pip install transformers datasets gdown\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import gdown\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, TextClassificationPipeline\n",
    "\n",
    "## 1 - Example using an existing sentiment analysis dataset\n",
    "\n",
    "### Training the model\n",
    "\n",
    "# Load the IMDB dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "imdb_dataset = imdb_dataset.map(tokenize, batched=True, batch_size=len(imdb_dataset[\"train\"]))\n",
    "imdb_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "train_dataset = imdb_dataset['train'].train_test_split(test_size=0.2)['train']\n",
    "test_dataset = imdb_dataset['train'].train_test_split(test_size=0.2)['test']\n",
    "val_dataset = test_dataset.train_test_split(test_size=0.5)['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\".\",\n",
    "  num_train_epochs=3,              # total number of training epochs\n",
    "  per_device_train_batch_size=16,  # batch size per device during training\n",
    "  per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "  weight_decay=0.01,               # strength of weight decay\n",
    "  logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "  args=training_args,                  # training arguments, defined above\n",
    "  train_dataset=train_dataset,         # training dataset\n",
    "  eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"imdb_classification\")\n",
    "\n",
    "### Evaluating the model\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# Tokenize and encode the input\n",
    "input_text = \"This movie was fantastic! I really enjoyed it.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Perform classification\n",
    "result = trainer.predict(test_dataset)\n",
    "labels = np.argmax(result.predictions, axis=1)\n",
    "print(classification_report(test_dataset['label'], labels, target_names=['negative', 'positive']))\n",
    "\n",
    "## 2 - Website Example\n",
    "\n",
    "### Obtaining the dataset\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=12c0cBC3U2kR976EZrlas4DJiOAC-z58u\"\n",
    "filename = \"website_categories.csv\"\n",
    "gdown.download(url, filename, quiet=False)\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Split the data into training, testing, and validation sets\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Write each set to a new CSV file\n",
    "train_file = 'train_' + filename\n",
    "test_file = 'test_' + filename\n",
    "train.to_csv(train_file, index=False)\n",
    "test.to_csv(test_file, index=False)\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "def extract_words_from_url(url):\n",
    "    url = re.sub(r'https?://', '', url)  # Remove 'http://' or 'https://'\n",
    "    url = re.sub(r'www\\.', '', url)  # Remove 'www.'\n",
    "    url = re.sub(r'\\.[a-zA-Z]+', '', url)  # Remove domain extension\n",
    "    url = re.sub(r'[-_/]', ' ', url)  # Replace '-', '_', and '/' with spaces\n",
    "    words = re.findall(r'\\b\\w+\\b', url)  # Extract words\n",
    "    return ' '.join(words).strip()\n",
    "\n",
    "def add_fields(df_tmp, le):\n",
    "  df_tmp['text'] = df_tmp['website_url'].apply(extract_words_from_url)\n",
    "  df_tmp['text'] += ' ' + df_tmp['cleaned_website_text']\n",
    "  df_tmp['label'] = le.transform(df_tmp['Category'])\n",
    "  return df_tmp\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df['Category'])\n",
    "classes = le.classes_\n",
    "num_categories = len(classes)\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "df_train = add_fields(df_train, le)\n",
    "df_test = add_fields(df_test, le)\n",
    "df_train.to_csv(train_file, index=False)\n",
    "df_test.to_csv(test_file, index=False)\n",
    "\n",
    "### Training the model\n",
    "\n",
    "# Load the dataset from CSV files\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": train_file, \"test\": test_file})\n",
    "\n",
    "# Define the model and tokenizer\n",
    "# model_name = \"microsoft/deberta-base\"\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize(batch):\n",
    "  return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=len(dataset[\"train\"]))\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "train_dataset = dataset['train'].train_test_split(test_size=0.2)['train']\n",
    "test_dataset = dataset['train'].train_test_split(test_size=0.2)['test']\n",
    "val_dataset = test_dataset.train_test_split(test_size=0.5)['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\".\",\n",
    "  num_train_epochs=3,              # total number of training epochs\n",
    "  per_device_train_batch_size=16,  # batch size per device during training\n",
    "  per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "  weight_decay=0.01,               # strength of weight decay\n",
    "  logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "  args=training_args,                  # training arguments, defined above\n",
    "  train_dataset=train_dataset,         # training dataset\n",
    "  eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"website_classification\")\n",
    "\n",
    "### Evaluating the model\n",
    "\n",
    "Evaluate the entire model\n",
    "\n",
    "# Perform classification\n",
    "result = trainer.predict(test_dataset)\n",
    "labels = np.argmax(result.predictions, axis=1)\n",
    "print(classification_report(test_dataset['label'], labels, target_names=classes))\n",
    "\n",
    "Evaluate a single string\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('/content/website_classification/')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=False)\n",
    "print(classes)\n",
    "pipe(\"youtube video learn\")\n",
    "\n",
    "## 3 - Applications classification\n",
    "\n",
    "### 3.1 - Obtaining the dataset\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=16gad7p-qxRxEoo_6r80oRYQOr3bFioQ8\"\n",
    "filename = \"apps_categories.csv\"\n",
    "gdown.download(url, filename, quiet=False)\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Split the data into training, testing, and validation sets\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Write each set to a new CSV file\n",
    "train_file = 'train_' + filename\n",
    "test_file = 'test_' + filename\n",
    "train.to_csv(train_file, index=False)\n",
    "test.to_csv(test_file, index=False)\n",
    "\n",
    "### 3.2 - Transforming the data\n",
    "\n",
    "category_column_name = 'Category'\n",
    "le = LabelEncoder()\n",
    "le.fit(df[category_column_name])\n",
    "\n",
    "def add_fields(df_tmp, le):\n",
    "  df_tmp['text'] = df_tmp['Name'] # Please define the columns to add, this will be a long text separated by spaces\n",
    "  df_tmp['label'] = le.transform(df_tmp[category_column_name])\n",
    "  return df_tmp\n",
    "\n",
    "classes = le.classes_\n",
    "num_categories = len(classes)\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "df_train = add_fields(df_train, le)\n",
    "df_test = add_fields(df_test, le)\n",
    "df_train.to_csv(train_file, index=False)\n",
    "df_test.to_csv(test_file, index=False)\n",
    "\n",
    "### 3.3 - Training the model\n",
    "\n",
    "# Load the dataset from CSV files\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": train_file, \"test\": test_file})\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize(batch):\n",
    "  return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=len(dataset[\"train\"]))\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "train_dataset = dataset['train'].train_test_split(test_size=0.2)['train']\n",
    "test_dataset = dataset['train'].train_test_split(test_size=0.2)['test']\n",
    "val_dataset = test_dataset.train_test_split(test_size=0.5)['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\".\",\n",
    "  num_train_epochs=3,              # total number of training epochs\n",
    "  per_device_train_batch_size=16,  # batch size per device during training\n",
    "  per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "  weight_decay=0.01,               # strength of weight decay\n",
    "  logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "  args=training_args,                  # training arguments, defined above\n",
    "  train_dataset=train_dataset,         # training dataset\n",
    "  eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"apps_classification\")\n",
    "\n",
    "### 3.4 - Evaluating the model\n",
    "\n",
    "Evaluate the entire model\n",
    "\n",
    "# Perform classification\n",
    "result = trainer.predict(test_dataset)\n",
    "labels = np.argmax(result.predictions, axis=1)\n",
    "print(classification_report(test_dataset['label'], labels))\n",
    "\n",
    "Evaluate a single String\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('/content/apps_classification/')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=False)\n",
    "print(classes)\n",
    "pipe(\"Microsoft Office Word\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
